{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MZ1cOcOviD9v"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pickle\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "YBF34He81ZaT"
      },
      "outputs": [],
      "source": [
        "# считает количество страниц внутри раздела\n",
        "def num_pages(link):\n",
        "    n = 1 + len(BeautifulSoup(requests.get(link).text, \"lxml\").findAll('a', class_ = 'post-page-numbers'))\n",
        "    return n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Y9cAbVCkBUCQ"
      },
      "outputs": [],
      "source": [
        "# очистка строки от ненужных нам символов\n",
        "def mysub(str):\n",
        "    return re.sub(r\"[\\d]+ | [a-zA-Z]+\", r\"\", a).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "MvQAwxjgH05D"
      },
      "outputs": [],
      "source": [
        "# проверка на наличие косяков сайта\n",
        "def check(lst1, lst2):\n",
        "    if (len(lst1) == len(lst2)) and ('' not in lst1) and ('' not in lst2):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o61JY5ITybhy"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/featztex/PsychoBot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "WDdeAgK-vlKj"
      },
      "outputs": [],
      "source": [
        "# подключаем список с ссылками на разделы сайта (заходит как в Сolab, так и в Jupyter Notebook)\n",
        "links = []\n",
        "with open('PsychoBot/parsing/data/links.pickle', 'rb') as f:\n",
        "    links = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "b4g2xj2h6-8d"
      },
      "outputs": [],
      "source": [
        "phrases = []\n",
        "authors = []\n",
        "\n",
        "for link in links:\n",
        "\n",
        "    for i in range(1, num_pages(link) + 1):\n",
        "\n",
        "        if i == 1:\n",
        "            page_url = link\n",
        "        else:\n",
        "            page_url = link + '/' + str(i)\n",
        "\n",
        "        # это отвратительная страница\n",
        "        if (page_url == 'https://uaforizm.com/vyskazyvanija-zhizni.html'):\n",
        "            continue\n",
        "        \n",
        "        page = requests.get(page_url)\n",
        "\n",
        "        soup = BeautifulSoup(page.text, \"lxml\")\n",
        "        \n",
        "        lst1 = soup.findAll('p')[:-2]\n",
        "        lst2 = soup.findAll('em')\n",
        "\n",
        "        for i in range(0, len(lst1)):\n",
        "            a = lst1[i].text\n",
        "            a = mysub(a)\n",
        "            lst1[i] = a\n",
        "        \n",
        "        for i in range(0, len(lst2)):\n",
        "            a = lst2[i].text\n",
        "            a = mysub(a)\n",
        "            lst2[i] = a\n",
        "\n",
        "        # удаляем имена авторов из списка lst1\n",
        "        i = 0\n",
        "        while i < len(lst1):\n",
        "            if lst1[i] in lst2:\n",
        "                del lst1[i]\n",
        "            else:\n",
        "                i += 1\n",
        "\n",
        "        if check(lst1, lst2):\n",
        "            phrases += lst1\n",
        "            authors += lst2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42Dor9ExldHE"
      },
      "outputs": [],
      "source": [
        "# собираем датасет\n",
        "ser1 = pd.Series(phrases)\n",
        "ser2 = pd.Series(authors)\n",
        "df = pd.DataFrame({'Цитата' : ser1, 'Автор' : ser2})\n",
        "df[\"id\"] = range(0, df.shape[0])\n",
        "df = df.reindex(columns=['id','Цитата','Автор'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "2iCY2ZPup7Wl"
      },
      "outputs": [],
      "source": [
        "df.to_csv('data.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "parser_phrases.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
